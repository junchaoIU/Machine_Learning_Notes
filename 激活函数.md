## 1 激活函数

动机：

- 数据角度：由于数据是线性不可分的，如果采用线性化，那么需要复杂的线性组合去逼近问题，因此需要非线性变换对数据分布进行重新映射;
- 线性模型的表达力问题：由于线性模型的表达能力不够，引入激活函数添加非线性因素

## 2 常见激活函数

### 2.1 sigmoid

- 公式：

$$
\sigma(x)=\frac {1}{1+e^{-x}}
$$

- 为什么选 sigmoid 函数 作为激活函数？

  sigmoid 函数 能够把输入的连续实值变换为0和1之间的输出，特别的，如果是非常大的负数，那么输出就是0；如果是非常大的正数，输出就是1.

### 2.2 tanh 

- 公式：

$$
tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}
$$

- 为什么选 tanh 函数 作为激活函数？

  tanh 函数 能够 解决 sigmoid 函数 非 0 均值 问题

### 2.3 relu

$$
f(x)=max(0, x)
$$

- 为什么选 relu 函数 作为激活函数？
  - 解决了gradient vanishing问题 (在正区间)
  - 计算速度非常快，只需要判断输入是否大于0
  - 收敛速度远快于sigmoid和tanh

## 如何选择合适的激活函数

- 深度学习往往需要大量的时间来处理数据，模型的收敛速度是尤为重要的，所以，在训练深度学习网络时尽量使用zero-centered特点的激活函数可以加快模型的收敛速度